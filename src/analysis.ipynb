{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAHWCAYAAAC7VLk1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSEUlEQVR4nO3de1hU1foH8O+AMNxklOSmIih4TRFDRch7KKl5pDpHUkv0eClDUzFLSkW0wmvRxSOmpaeSg9nxlhVmeMsgLyilphxRFDMGNXUQENCZ9fvDn5MjtxlmYGY738/z7Odp1qy997t3Iq/vWnttmRBCgIiIiMjC2Zg7ACIiIiJ9MGkhIiIiSWDSQkRERJLApIWIiIgkgUkLERERSQKTFiIiIpIEJi1EREQkCUxaiIiISBKYtBAREZEkMGkhesjIZDIsWLDA3GGY3Lhx4+Dn51fnfV1cXEwbEBE1OCYtRPf517/+BZlMhpCQkCq//+2337BgwQKcP3++yn3Xr19fvwH+v2+//fahTEzMrbS0FAsWLMDevXvNHQoRVYFJC9F9NmzYAD8/Pxw6dAi5ubmVvv/tt9+QkJBgEUlLQkJCld/dunULc+fObZA4GtKaNWuQk5NTr+coLS1FQkICkxYiC8Wkhej/5eXlISMjA++++y7c3d2xYcMGc4dUJw4ODmjUqJG5wzA5Ozs7yOVyc4dBRGbEpIXo/23YsAFNmzbFsGHD8Pe//71S0rJ+/Xr84x//AAAMGDAAMpkMMpkMe/fuhZ+fH06ePIl9+/Zp2/v376/d98aNG5gxYwZ8fHwgl8sREBCAJUuWQKPRaPucP38eMpkMy5cvx8cffwx/f3/I5XL06NEDhw8f1vYbN24cVq5cCQDac8lkMu33Vc1pOXbsGIYMGQJXV1e4uLjgiSeewM8//1zp+mQyGX766SfExsbC3d0dzs7OePrpp3HlypUa79327dshk8nw66+/atv++9//QiaT4ZlnntHp27FjR0RFRem0ffHFFwgODoajoyPc3Nzw3HPP4eLFizp9qprT8ueff+KFF16Aq6srmjRpgujoaPzyyy+QyWRVVr0uXbqEyMhIuLi4wN3dHa+++irUajWAu/ff3d0dAJCQkKC9r/fupVKpxPjx49GyZUvI5XJ4e3tjxIgRVVbdiKh+PHz/HCOqow0bNuCZZ56Bvb09Ro0ahVWrVuHw4cPo0aMHAKBv37545ZVX8MEHH+CNN95Ax44dAdz9JZyUlIRp06bBxcUFb775JgDA09MTwN0hh379+uHSpUt48cUX0apVK2RkZCAuLg4FBQVISkrSiSMlJQU3b97Eiy++CJlMhqVLl+KZZ57BuXPnYGdnhxdffBF//PEHdu3ahc8//7zW6zp58iT69OkDV1dXvPbaa7Czs8Pq1avRv39/7Nu3r9L8nWnTpqFp06aIj4/H+fPnkZSUhKlTp2Ljxo3VnqN3796QyWTYv38/AgMDAQA//vgjbGxscODAAW2/K1eu4PTp05g6daq27e2338a8efMwcuRITJw4EVeuXMGHH36Ivn374tixY2jSpEmV59RoNBg+fDgOHTqEKVOmoEOHDti2bRuio6Or7K9WqxEREYGQkBAsX74cP/zwA1asWAF/f39MmTIF7u7uWLVqFaZMmYKnn35am2zdu55nn30WJ0+exLRp0+Dn54fLly9j165dyM/Pr/MEYSIykCAiceTIEQFA7Nq1SwghhEajES1bthTTp0/X6bdp0yYBQOzZs6fSMR599FHRr1+/Su2LFi0Szs7O4n//+59O+5w5c4Stra3Iz88XQgiRl5cnAIhHHnlEXLt2Tdtv27ZtAoD4+uuvtW0xMTGiuh9fACI+Pl77OTIyUtjb24uzZ89q2/744w/RuHFj0bdvX23bunXrBAARHh4uNBqNtn3mzJnC1tZW3Lhxo8rz3X/9I0eO1H5+7LHHxD/+8Q8BQJw6dUoIIcTmzZsFAPHLL78IIYQ4f/68sLW1FW+//bbOsY4fPy4aNWqk0x4dHS18fX21n//73/8KACIpKUnbplarxcCBAwUAsW7dOp19AYiFCxfqnKdbt24iODhY+/nKlSuV7p8QQly/fl0AEMuWLavxHhBR/eLwEBHuVlk8PT0xYMAAAHeHWKKiopCamqodPqirTZs2oU+fPmjatCmuXr2q3cLDw6FWq7F//36d/lFRUWjatKn2c58+fQAA586dM/jcarUa33//PSIjI9GmTRttu7e3N0aPHo0DBw6gqKhIZ5/JkyfrDDf16dMHarUaFy5cqPFcffr0wY8//ggAuHnzJn755RdMnjwZzZo107b/+OOPaNKkCTp37gwA2Lx5MzQaDUaOHKlzb7y8vNC2bVvs2bOn2vOlpaXBzs4OkyZN0rbZ2NggJiam2n1eeumlSjHrc18dHR1hb2+PvXv34vr167X2J6L6waSFrJ5arUZqaioGDBiAvLw85ObmIjc3FyEhISgsLER6erpRxz9z5gzS0tLg7u6us4WHhwMALl++rNO/VatWOp/vJTB1+WV55coVlJaWon379pW+69ixIzQaTaW5I3U9f58+fVBQUIDc3FxkZGRAJpMhNDRUJ5n58ccf8fjjj8PG5u5fPWfOnIEQAm3btq10f06dOlXp3tzvwoUL8Pb2hpOTk057QEBAlf0dHBy0c1buvzZ97qtcLseSJUvw3XffwdPTE3379sXSpUuhVCpr3ZeITIdzWsjq7d69GwUFBUhNTUVqamql7zds2IDBgwfX+fgajQaDBg3Ca6+9VuX37dq10/lsa2tbZT8hRJ1jMERdz9+7d28AwP79+3Hu3Dk89thjcHZ2Rp8+ffDBBx+guLgYx44dw9tvv63dR6PRQCaT4bvvvqvyvKZcEK6669LXjBkzMHz4cGzduhU7d+7EvHnzkJiYiN27d6Nbt24mipKIasKkhazehg0b4OHhoX0i536bN2/Gli1bkJycDEdHR51hkwdV952/vz+Ki4u1lRVTqCmO+7m7u8PJyanK9U1Onz4NGxsb+Pj4mCSmVq1aoVWrVvjxxx9x7tw57bBW3759ERsbi02bNkGtVqNv377affz9/SGEQOvWrSslb7Xx9fXFnj17UFpaqlNtqWp9HX3Vdl/9/f0xa9YszJo1C2fOnEFQUBBWrFiBL774os7nJCL9cXiIrNqtW7ewefNmPPXUU/j73/9eaZs6dSpu3ryJ7du3AwCcnZ0B3H2E+UHOzs5Vto8cORKZmZnYuXNnpe9u3LiBO3fuGBx3TXHcz9bWFoMHD8a2bdt0Hs0tLCxESkoKevfuDVdXV4PPX50+ffpg9+7dOHTokDZpCQoKQuPGjbF48WI4OjoiODhY2/+ZZ56Bra0tEhISKlVyhBD4888/qz1XREQEbt++jTVr1mjbNBpNlcmnvu4lPw/e19LSUpSVlem0+fv7o3HjxigvL6/z+YjIMKy0kFXbvn07bt68ib/97W9Vft+rVy/tQnNRUVEICgqCra0tlixZApVKBblcjoEDB8LDwwPBwcFYtWoV3nrrLQQEBMDDwwMDBw7E7NmzsX37djz11FMYN24cgoODUVJSguPHj+Orr77C+fPn0axZM4PivveL/5VXXkFERARsbW3x3HPPVdn3rbfewq5du9C7d2+8/PLLaNSoEVavXo3y8nIsXbrUsBtWiz59+mDDhg2QyWTa4SJbW1uEhYVh586d6N+/P+zt7bX9/f398dZbbyEuLg7nz59HZGQkGjdujLy8PGzZsgWTJ0/Gq6++WuW5IiMj0bNnT8yaNQu5ubno0KEDtm/fjmvXrgHQvxp1P0dHR3Tq1AkbN25Eu3bt4Obmhs6dO+POnTt44oknMHLkSHTq1AmNGjXCli1bUFhYWO19J6J6YM5Hl4jMbfjw4cLBwUGUlJRU22fcuHHCzs5OXL16VQghxJo1a0SbNm2Era2tzuPPSqVSDBs2TDRu3FgA0Hn8+ebNmyIuLk4EBAQIe3t70axZMxEWFiaWL18uKioqhBB/PfJc1WO1eOAx3Dt37ohp06YJd3d3IZPJdB5/frCvEEIcPXpURERECBcXF+Hk5CQGDBggMjIydPrce+T58OHDOu179uyp9jHvB508eVIAEB07dtRpf+uttwQAMW/evCr3++9//yt69+4tnJ2dhbOzs+jQoYOIiYkROTk52j4PPvIsxN1HlEePHi0aN24sFAqFGDdunPjpp58EAJGamqqzr7Ozc6XzxsfHV3p0PCMjQwQHBwt7e3vtvbx69aqIiYkRHTp0EM7OzkKhUIiQkBDx5Zdf1npPiMh0ZEI00Ow+IqIGsHXrVjz99NM4cOAAHn/8cXOHQ0QmxKSFiCTr1q1bcHR01H5Wq9UYPHgwjhw5AqVSqfMdEUkf57QQkWRNmzYNt27dQmhoKMrLy7F582ZkZGTgnXfeYcJC9BBipYWIJCslJQUrVqxAbm4uysrKEBAQgClTpui824iIHh6SeeT52rVrGDNmjPZtrhMmTEBxcXGN+/Tv31/nLbgymazSMt5EJF2jR49GVlYWVCoVysvLcfLkSSYsRA8xyVRahgwZgoKCAqxevRq3b9/G+PHj0aNHD6SkpFS7T//+/dGuXTssXLhQ2+bk5GTSdSmIiIioYUhiTsupU6eQlpaGw4cPo3v37gCADz/8EEOHDsXy5cvRvHnzavd1cnKCl5dXQ4VKRERE9UQSSUtmZiaaNGmiTVgAIDw8HDY2Njh48CCefvrpavfdsGEDvvjiC3h5eWH48OGYN29epRes3a+8vFxnhUuNRoNr167hkUceqdNiVUREJF1CCNy8eRPNmzfXvujTkpSVlaGiosIkx7K3t4eDg4NJjlVfJJG0KJVKeHh46LQ1atQIbm5uNb5ldfTo0fD19UXz5s3x66+/4vXXX0dOTg42b95c7T6JiYlISEgwWexERCR9Fy9eRMuWLc0dho6ysjK0bt3aZG8b9/LyQl5enkUnLmZNWubMmYMlS5bU2OfUqVN1Pv7kyZO1/92lSxd4e3vjiSeewNmzZ+Hv71/lPnFxcYiNjdV+VqlUaNWqFS5evMi5MGbipVCYOwQislICQBmAxo0bmzuUSioqKqBUKnHxYp7Rv5+Kiorg49MaFRUVTFqqM2vWLIwbN67GPm3atIGXlxcuX76s037nzh1cu3bNoPkqISEhAO6+Bba6pEUul0Mul1dqd3V1ZdJiJhyUIyJzs+TpAdb0+8msSYu7uzvc3d1r7RcaGoobN24gKytL+6K43bt3Q6PRaBMRfWRnZwMAvL296xQvERGR5bnz/5uxx7B8ljerqAodO3bEk08+iUmTJuHQoUP46aefMHXqVDz33HPaJ4cuXbqEDh064NChQwCAs2fPYtGiRcjKysL58+exfft2jB07Fn379kVgYKA5L4eIiMiE7phos3ySSFqAu08BdejQAU888QSGDh2K3r174+OPP9Z+f/v2beTk5KC0tBTA3VnQP/zwAwYPHowOHTpg1qxZePbZZ/H111+b6xKIiIjICJJZXM5cioqKoFAooFKprGbM0NI4W/BYMhE93ASAW4BF/g746/fTBZNMxFUofC3yOu8niUeeiYiIqDpqGD+8ozZFIPVOMsNDREREZN1YaSEiIpI063l6iEkLERGRpFlP0sLhISIiIpIEVlqIiIgkzXoqLUxaiIiIJE0N45/+4dNDRERERCbDSgsREZGkWc86LUxaiIiIJM165rRweIiIiIgkgZUWIiIiSbOeSguTFiIiIkmznqSFw0NEREQkCay0EBERSRqfHiIiIiJJ4PAQERERkUVhpYWIiEjSrKfSwqSFiIhI0qwnaeHwEBEREUkCKy1ERESSZj2VFiYtREREkmY9jzxzeIiIiIgkgZUWIiIiSePwEBEREUmC9SQtHB4iIiIiSWClhYiISNKsp9LCpIWIiEjSrCdp4fAQERERSQIrLURERJJmPeu0MGkhIiKSNDWMTzqkkbRweIiIiIgkgZUWIiIiSbOeibhMWoiIiCTNepIWDg8RERGRJLDSQkREJGl8eoiIiIgkgcNDFmvlypXw8/ODg4MDQkJCcOjQoRr7b9q0CR06dICDgwO6dOmCb7/9toEiJSIiIlOSVNKyceNGxMbGIj4+HkePHkXXrl0RERGBy5cvV9k/IyMDo0aNwoQJE3Ds2DFERkYiMjISJ06caODIiYiI6ssdE22WTyaEEOYOQl8hISHo0aMHPvroIwCARqOBj48Ppk2bhjlz5lTqHxUVhZKSEuzYsUPb1qtXLwQFBSE5OVmvcxYVFUGhUEClUsHV1dU0F0IGcZbJzB0CEVkpAeAWYJG/A/76/TQPrq4ORh6rDArFIou8zvtJptJSUVGBrKwshIeHa9tsbGwQHh6OzMzMKvfJzMzU6Q8AERER1fYHgPLychQVFelsREREZH6SSVquXr0KtVoNT09PnXZPT08olcoq91EqlQb1B4DExEQoFArt5uPjY3zwRERE9cZ6hockk7Q0lLi4OKhUKu128eJFc4dERERUg3uPPBuz8ZFnk2rWrBlsbW1RWFio015YWAgvL68q9/Hy8jKoPwDI5XLI5XLjAyYiIiKTkkylxd7eHsHBwUhPT9e2aTQapKenIzQ0tMp9QkNDdfoDwK5du6rtT0REJD3WMzwkmUoLAMTGxiI6Ohrdu3dHz549kZSUhJKSEowfPx4AMHbsWLRo0QKJiYkAgOnTp6Nfv35YsWIFhg0bhtTUVBw5cgQff/yxOS+DiIjIhO4AsDXBMSyfpJKWqKgoXLlyBfPnz4dSqURQUBDS0tK0k23z8/NhY/NX8SgsLAwpKSmYO3cu3njjDbRt2xZbt25F586dzXUJREREVEeSWqfFHLhOi/lxnRYiMhdprNPyMlxdjZuLWVRUDoXiXxZ5nfeTVKWFiIiIHmQ9L0yUzERcIiIism6stBAREUnaHRhfg+BEXCIiIqp31pO0cHiIiIiIJIGVFiIiIkmznkoLkxYiIiJJU8P4p3/49BARERGRybDSQkREJGnWs04LkxYiIiJJuwPA2JXDpTGnhcNDREREJAmstBAREUma9VRamLQQERFJmvUkLRweIiIiIklgpYWIiEjSrKfSwqSFiIhI0tQwPmmRxiPPHB4iIiIiSWClhYiISNJMMbQjjeEhVlqIiIgk7Y6JNsOsXLkSfn5+cHBwQEhICA4dOlRj/6SkJLRv3x6Ojo7w8fHBzJkzUVZWZtA5mbQQERGRQTZu3IjY2FjEx8fj6NGj6Nq1KyIiInD58uUq+6ekpGDOnDmIj4/HqVOn8Mknn2Djxo144403DDovkxYiIiJJa/hKy7vvvotJkyZh/Pjx6NSpE5KTk+Hk5IRPP/20yv4ZGRl4/PHHMXr0aPj5+WHw4MEYNWpUrdWZBzFpISIikrR7L0w0Zrv79FBRUZHOVl5eXulsFRUVyMrKQnh4uLbNxsYG4eHhyMzMrDLCsLAwZGVlaZOUc+fO4dtvv8XQoUMNulImLURERAQA8PHxgUKh0G6JiYmV+ly9ehVqtRqenp467Z6enlAqlVUed/To0Vi4cCF69+4NOzs7+Pv7o3///gYPD/HpISIiIkm7A0AYeYy7lZaLFy/C1dVV2yqXy4087l179+7FO++8g3/9618ICQlBbm4upk+fjkWLFmHevHl6H4dJCxERkaSZLmlxdXXVSVqq0qxZM9ja2qKwsFCnvbCwEF5eXlXuM2/ePLzwwguYOHEiAKBLly4oKSnB5MmT8eabb8LGRr+BHw4PERERkd7s7e0RHByM9PR0bZtGo0F6ejpCQ0Or3Ke0tLRSYmJrawsAEEL/hIuVFiIiIkkzXaVFX7GxsYiOjkb37t3Rs2dPJCUloaSkBOPHjwcAjB07Fi1atNDOiRk+fDjeffdddOvWTTs8NG/ePAwfPlybvOiDSQsREZGkNXzSEhUVhStXrmD+/PlQKpUICgpCWlqadnJufn6+TmVl7ty5kMlkmDt3Li5dugR3d3cMHz4cb7/9tkHnlQlD6jJWqKioCAqFAiqVqtZxPqofzjJjXwRGRFQ3AsAtwCJ/B/z1+8kfrq76VyuqPpYaCsVZi7zO+7HSQkREJGlqGF9p0ZgikHrHpIWIiEjSrCdp4dNDREREJAmstBAREUnaHRhfg5BGpYVJCxERkaRZT9LC4SEiIiKSBFZaiIiIJM16Ki1MWoiIiCRNDeOTDmks2Sa54aGVK1fCz88PDg4OCAkJwaFDh6rtu379eshkMp3NwcGhAaMlIiIiU5FUpWXjxo2IjY1FcnIyQkJCkJSUhIiICOTk5MDDw6PKfVxdXZGTk6P9LOPqqkRE9FC5A8DY322stJjcu+++i0mTJmH8+PHo1KkTkpOT4eTkhE8//bTafWQyGby8vLTbvfciEBERPRzumGizfJJJWioqKpCVlYXw8HBtm42NDcLDw5GZmVntfsXFxfD19YWPjw9GjBiBkydP1nie8vJyFBUV6WxERERkfpIZHrp69SrUanWlSomnpydOnz5d5T7t27fHp59+isDAQKhUKixfvhxhYWE4efIkWrZsWeU+iYmJSEhIqNTupVAYXXyjuinhOz3Nii+sJLJ0HB56KISGhmLs2LEICgpCv379sHnzZri7u2P16tXV7hMXFweVSqXdLl682IARExERGUhoAKE2cuMjzybVrFkz2NraorCwUKe9sLAQXl5eeh3Dzs4O3bp1Q25ubrV95HI55HK5UbESERGR6Umm0mJvb4/g4GCkp6dr2zQaDdLT0xEaGqrXMdRqNY4fPw5vb+/6CpOIiKhhaUy0SYBkKi0AEBsbi+joaHTv3h09e/ZEUlISSkpKMH78eADA2LFj0aJFCyQmJgIAFi5ciF69eiEgIAA3btzAsmXLcOHCBUycONGcl0FERGQ66v/fjD2GBEgqaYmKisKVK1cwf/58KJVKBAUFIS0tTTs5Nz8/HzY2fxWPrl+/jkmTJkGpVKJp06YIDg5GRkYGOnXqZK5LICIiojqSCcFHM2pSVFQEhUIBRxg/N5vqhk8PmRefHiJrJgDcAqBSqeDq6mrucHTc+/2kUgLGhlZUBCi8LPM67yepSgsRERE9wBRzUiQyp0UyE3GJiIjIurHSQkREJGWciEtERESSwOEhIiIiIsvCSgsREZGUaWD88A4rLURERESmw0oLERGRlHEiLhEREUkCJ+ISERERWRZWWoiIiKSMw0NEREQkCVaUtHB4iIiIiCSBlRYiIiIps6KJuExaiIiIpIzDQ0RERESWhZUWIiIiKRMwfnhHmCKQ+sekhYiISMo4PERERERkWVhpISIikjIrqrQwaSEiIpIyK3rkmcNDREREJAmstBAREUkZh4eIiIhIEqwoaeHwEBEREUkCKy1ERERSZkUTcZm0EBERSZkGxg/vSCRp4fAQERERSQIrLURERFLG4SEiIiKSBD49RERERGRZWGkhIiKSMiuqtDBpISIikjIrmtNikuGhGzdumOIwRERERNUyOGlZsmQJNm7cqP08cuRIPPLII2jRogV++eUXkwZHREREtVCbaJMAg5OW5ORk+Pj4AAB27dqFXbt24bvvvsOQIUMwe/ZskwdIRERENbCipMXgOS1KpVKbtOzYsQMjR47E4MGD4efnh5CQEJMHSERERATUodLStGlTXLx4EQCQlpaG8PBwAIAQAmp1/aZq+/fvx/Dhw9G8eXPIZDJs3bq11n327t2Lxx57DHK5HAEBAVi/fn29xkhERNSgBP6ajFvXTTR41HVicNLyzDPPYPTo0Rg0aBD+/PNPDBkyBABw7NgxBAQEmDzA+5WUlKBr165YuXKlXv3z8vIwbNgwDBgwANnZ2ZgxYwYmTpyInTt31mucREREDYbDQ9V777334Ofnh4sXL2Lp0qVwcXEBABQUFODll182eYD3GzJkiDZJ0kdycjJat26NFStWAAA6duyIAwcO4L333kNERER9hUlERET1wOCkxc7ODq+++mql9pkzZ5okIFPKzMzUDl/dExERgRkzZlS7T3l5OcrLy7Wfi4qK6is8IiIi43Gdlpp9/vnn6N27N5o3b44LFy4AAJKSkrBt2zaTBmcspVIJT09PnTZPT08UFRXh1q1bVe6TmJgIhUKh3e5NOiYiIrJIVjQ8ZHDSsmrVKsTGxmLIkCG4ceOGdvJtkyZNkJSUZOr4GlxcXBxUKpV2uzfpmIiIiMzL4KTlww8/xJo1a/Dmm2/C1tZW2969e3ccP37cpMEZy8vLC4WFhTpthYWFcHV1haOjY5X7yOVyuLq66mxEREQWy4oqLQbPacnLy0O3bt0qtcvlcpSUlJgkKFMJDQ3Ft99+q9O2a9cuhIaGmikiIiIiE+Ocluq1bt0a2dnZldrT0tLQsWNHU8RUreLiYmRnZ2vPn5eXh+zsbOTn5wO4O7QzduxYbf+XXnoJ586dw2uvvYbTp0/jX//6F7788kuLnDRMRERENTO40hIbG4uYmBiUlZVBCIFDhw7hP//5DxITE7F27dr6iFHryJEjGDBggE4sABAdHY3169ejoKBAm8AAdxOsb775BjNnzsT777+Pli1bYu3atXzcmYiIHh6mGN6RyPCQTAhh8Dp4GzZswIIFC3D27FkAQPPmzZGQkIAJEyaYPEBzKyoqgkKhgCMAmbmDsVIlhv8RJRNylvFPPlkvAeAWAJVKZXFzHO/9flKtBlyrnqap/7FuAYoXDbvOlStXYtmyZVAqlejatSs+/PBD9OzZs9r+N27cwJtvvonNmzfj2rVr8PX1RVJSEoYOHap3nAZVWu7cuYOUlBRERERgzJgxKC0tRXFxMTw8PAw5DBEREUnYxo0bERsbi+TkZISEhCApKQkRERHIycmpMieoqKjAoEGD4OHhga+++gotWrTAhQsX0KRJE4POa3ClxcnJCadOnYKvr69BJ5IqVlrMj5UW82KlhayZJCotq0xUaZmi/3WGhISgR48e+OijjwAAGo0GPj4+mDZtGubMmVOpf3JyMpYtW4bTp0/Dzs6uznEaPBG3Z8+eOHbsWJ1PSERERCZkwkeei4qKdLb7V4i/p6KiAllZWTorztvY2CA8PByZmZlVhrh9+3aEhoYiJiYGnp6e6Ny5M9555x2DX7Rs8ETcl19+GbNmzcLvv/+O4OBgODs763wfGBho6CGJiIjIAjy4Cnx8fDwWLFig03b16lWo1eoqV5w/ffp0lcc9d+4cdu/ejTFjxuDbb79Fbm4uXn75Zdy+fRvx8fF6x2dw0vLcc88BAF555RVtm0wmgxACMpnM4KyJiIiIjGDCdVouXryoMzwkl8uNPPD/H16jgYeHBz7++GPY2toiODgYly5dwrJly+o3acnLyzN0FyIiIqovJnzkWZ+V4Js1awZbW9sqV5z38vKqch9vb2/Y2dnprKTfsWNHKJVKVFRUwN7eXq8wDU5arGUCLhEREVVmb2+P4OBgpKenIzIyEsDdSkp6ejqmTp1a5T6PP/44UlJSoNFoYGNzdzrt//73P3h7e+udsAB1SFo+++yzGr+/f0VaIiIiqmdmWFwuNjYW0dHR6N69O3r27ImkpCSUlJRg/PjxAO7mAi1atEBiYiIAYMqUKfjoo48wffp0TJs2DWfOnME777yjM9VEHwYnLdOnT9f5fPv2bZSWlsLe3h5OTk5MWoiIiBqSGd49FBUVhStXrmD+/PlQKpUICgpCWlqadnJufn6+tqIC3J3gu3PnTsycOROBgYFo0aIFpk+fjtdff92g89ZpRdwHnTlzBlOmTMHs2bMfuiXyuU6L+XGdFvPiOi1kzSSxTstSE63T8pplXuf9DF6npSpt27bF4sWLK1VhiIiIqJ5pYPwaLRJ5y7PBw0PVHqhRI/zxxx+mOhwRERHpwwzDQ+ZicNKyfft2nc9CCBQUFOCjjz7C448/brLAiIiIiO5ncNJy7/Gme2QyGdzd3TFw4ECsWLHCVHERERGRPszw9JC5GJy0aDQSqSERERFZAytKWgyeiLtw4UKUlpZWar916xYWLlxokqCIiIiIHmRw0pKQkIDi4uJK7aWlpUhISDBJUERERKQnjYk2CTB4eOjeixEf9Msvv8DNzc0kQREREZGerGh4SO+kpWnTppDJZJDJZGjXrp1O4qJWq1FcXIyXXnqpXoIkIiIi0jtpSUpKghAC//znP5GQkACFQqH9zt7eHn5+fggNDa2XIImIiKgarLRUFh0dDQBo3bo1wsLCYGdnV29BERERkZ4EjJ+TIpG3pRg8p6Vfv37a/y4rK0NFRYXO95b8zgIiIiKSLoOfHiotLcXUqVPh4eEBZ2dnNG3aVGcjIiKiBmTse4dMMbzUQAxOWmbPno3du3dj1apVkMvlWLt2LRISEtC8eXN89tln9REjERERVYePPFfv66+/xmeffYb+/ftj/Pjx6NOnDwICAuDr64sNGzZgzJgx9REnERERWTmDKy3Xrl1DmzZtANydv3Lt2jUAQO/evbF//37TRkdEREQ14/BQ9dq0aYO8vDwAQIcOHfDll18CuFuBadKkiUmDIyIiolowaane+PHj8csvvwAA5syZg5UrV8LBwQEzZ87E7NmzTR4gEREREVCHOS0zZ87U/nd4eDhOnz6NrKwsBAQEIDAw0KTBERERUS1MMZH2YZ2Ie7+ysjL4+vrC19fXVPEQERGRIaxoRVyDh4fUajUWLVqEFi1awMXFBefOnQMAzJs3D5988onJAyQiIiIC6pC0vP3221i/fj2WLl0Ke3t7bXvnzp2xdu1akwZHREREtdDA+Em4EhkeMjhp+eyzz/Dxxx9jzJgxsLW11bZ37doVp0+fNmlwREREVAsrWlzO4KTl0qVLCAgIqNSu0Whw+/ZtkwRFRERE9CCDk5ZOnTrhxx9/rNT+1VdfoVu3biYJioiIiPRkReu0GPz00Pz58xEdHY1Lly5Bo9Fg8+bNyMnJwWeffYYdO3bUR4xERERUHSt65NngSsuIESPw9ddf44cffoCzszPmz5+PU6dO4euvv8agQYPqI0YiIiIi/ZKWDz74AGVlZQCA/Px89O7dG7t27cLly5dRWlqKAwcOYPDgwfUaKADs378fw4cPR/PmzSGTybB169Ya++/duxcymazSplQq6z1WIiKiBmFFw0N6JS2xsbEoKioCALRu3RpXrlyp16CqU1JSgq5du2LlypUG7ZeTk4OCggLt5uHhUU8REhERNTArSlr0mtPSvHlz/Pe//8XQoUMhhMDvv/+urbw8qFWrViYN8H5DhgzBkCFDDN7Pw8ODL3MkIiKSOL2Slrlz52LatGmYOnUqZDIZevToUamPEAIymQxqteWla0FBQSgvL0fnzp2xYMECPP7449X2LS8vR3l5ufbzvQoTERGRRbKiibh6JS2TJ0/GqFGjcOHCBQQGBuKHH37AI488Ut+xGc3b2xvJycno3r07ysvLsXbtWvTv3x8HDx7EY489VuU+iYmJSEhIaOBIqSbOMpm5Q7BqJUKYOwSrx58BqtG9FXGNPYYEyIQw7G+kf//733juuecgl8vrKya9yGQybNmyBZGRkQbt169fP7Rq1Qqff/55ld9XVWnx8fGBIwD+tUHWiEmL+TFpMR8B4BYAlUoFV1dXc4ejo6ioCAqFAqq/A652Rh7rNqD4yjKv834Gr9MSHR1dH3E0mJ49e+LAgQPVfi+Xy82ekBEREelNjTosYFLFMSTA4KRF6rKzs+Ht7W3uMIiIiEyDc1osU3FxMXJzc7Wf8/LykJ2dDTc3N7Rq1QpxcXG4dOkSPvvsMwBAUlISWrdujUcffRRlZWVYu3Ytdu/eje+//95cl0BERER1JKmk5ciRIxgwYID2c2xsLIC7Q1br169HQUEB8vPztd9XVFRg1qxZuHTpEpycnLSTiO8/BhERkaRZ0fCQQRNxb9++jQ4dOmDHjh3o2LFjfcZlMe5NdOJEXLJWnIhrfpyIaz6SmIg71EQTcb+1zOu8n0G5mZ2dXbWLyhERERHVJ4MLSjExMViyZAnu3LlTH/EQERGRIbiMf/UOHz6M9PR0fP/99+jSpQucnZ11vt+8ebPJgiMiIqJaWNGcFoOTliZNmuDZZ5+tj1iIiIiIqmVw0rJu3br6iIOIiIjqQsD4dVYkMt++TgWlO3fu4IcffsDq1atx8+ZNAMAff/yB4uJikwZHREREteCclupduHABTz75JPLz81FeXo5BgwahcePGWLJkCcrLy5GcnFwfcRIREZGVM7jSMn36dHTv3h3Xr1+Ho6Ojtv3pp59Genq6SYMjIiKiWrDSUr0ff/wRGRkZsLe312n38/PDpUuXTBYYERER6cGK3j1kcKVFo9FAra6ckv3+++9o3LixSYIiIiIiepDBScvgwYORlJSk/SyTyVBcXIz4+HgMHTrUlLERERFRbTg8VL0VK1YgIiICnTp1QllZGUaPHo0zZ86gWbNm+M9//lMfMRIREVF1rGh4yOCkpWXLlvjll1+QmpqKX3/9FcXFxZgwYQLGjBmjMzGXiIiIyJQMTloAoFGjRnj++edNHQsREREZSg3A2BeBP6zDQwBw5swZ7NmzB5cvX4ZGo1tTmj9/vkkCIyIiIj1oYHzS8bAOD61ZswZTpkxBs2bN4OXlBZnsr/ROJpMxaSEiIqJ6YXDS8tZbb+Htt9/G66+/Xh/xEBERkSE0MH546GGttFy/fh3/+Mc/6iMWIiIiMpQp5qNIZE6Lweu0/OMf/8D3339fH7EQERERVUuvSssHH3yg/e+AgADMmzcPP//8M7p06QI7Ozudvq+88oppIyQiIqLqWVGlRSaEELV1at26tX4Hk8lw7tw5o4OyJEVFRVAoFHCE8UOGRFJUUvtfEVTPnGX828dcBIBbAFQqFVxdXc0djo57v59U7QFXWyOPpQYUOZZ5nffTq9KSl5dX33EQERER1ahO67QQERGRhbCi4SGDJ+I+++yzWLJkSaX2pUuX8qkiIiKihqYx0SYBBict+/fvr/JtzkOGDMH+/ftNEhQRERHRgwweHiouLoa9vX2ldjs7OxQVFZkkKCIiItKTKaokD2ulpUuXLti4cWOl9tTUVHTq1MkkQREREZGe1CbaJMDgSsu8efPwzDPP4OzZsxg4cCAAID09Hf/5z3+wadMmkwdIREREBNSh0jJ8+HBs3boVubm5ePnllzFr1iz8/vvv+OGHHxAZGVkPIRIREVG1zDQRd+XKlfDz84ODgwNCQkJw6NAhvfZLTU2FTCarU85Qp0eehw0bhmHDhtVlVyIiIjIlNe6ugmcMA5OWjRs3IjY2FsnJyQgJCUFSUhIiIiKQk5MDDw+Pavc7f/48Xn31VfTp06dOYRpcaSEiIiLr9u6772LSpEkYP348OnXqhOTkZDg5OeHTTz+tdh+1Wo0xY8YgISEBbdq0qdN5DU5a1Go1li9fjp49e8LLywtubm46GxERETUgE07ELSoq0tnKy8srna6iogJZWVkIDw/XttnY2CA8PByZmZnVhrlw4UJ4eHhgwoQJdb5Ug5OWhIQEvPvuu4iKioJKpUJsbCyeeeYZ2NjYYMGCBXUOhIiIiOrAhHNafHx8oFAotFtiYmKl0129ehVqtRqenp467Z6enlAqlVWGeODAAXzyySdYs2aNUZdq8JyWDRs2YM2aNRg2bBgWLFiAUaNGwd/fH4GBgfj555/5lmciIiKJunjxos4LE+VyudHHvHnzJl544QWsWbMGzZo1M+pYBictSqUSXbp0AQC4uLhApVIBAJ566inMmzfPqGCIiIjIQBoYPxH3//d3dXWt9S3PzZo1g62tLQoLC3XaCwsL4eXlVan/2bNncf78eQwfPvyvkDV3SzuNGjVCTk4O/P399QrT4OGhli1boqCgAADg7++P77//HgBw+PBhk2RkREREZIAGfuTZ3t4ewcHBSE9P/ysEjQbp6ekIDQ2t1L9Dhw44fvw4srOztdvf/vY3DBgwANnZ2fDx8dH73AZXWp5++mmkp6cjJCQE06ZNw/PPP49PPvkE+fn5mDlzpqGHIyIiIomJjY1FdHQ0unfvjp49eyIpKQklJSUYP348AGDs2LFo0aIFEhMT4eDggM6dO+vs36RJEwCo1F4bg5OWxYsXa/87KioKrVq1QmZmJtq2batT+jG1xMREbN68GadPn4ajoyPCwsKwZMkStG/fvsb9Nm3ahHnz5uH8+fNo27YtlixZUuULH4mIiCRJDUBm5DEMHF6KiorClStXMH/+fCiVSgQFBSEtLU07OTc/Px82NqZfVUUmhDB2JKxBPPnkk3juuefQo0cP3LlzB2+88QZOnDiB3377Dc7OzlXuk5GRgb59+yIxMRFPPfUUUlJSsGTJEhw9elTv7K6oqAgKhQKOMP7PBJEUlUjjr4iHmrOMf/uYiwBwC4BKpap1rkdDu/f7SeUIuBr5R6RIAIpblnmd96tT0vL5558jOTkZeXl5yMzMhK+vL5KSktC6dWuMGDGiPuKs5MqVK/Dw8MC+ffvQt2/fKvtERUWhpKQEO3bs0Lb16tULQUFBSE5O1us8TFrI2jFpMT8mLebDpMWyGFy7WbVqFWJjYzF06FDcuHEDavXdFWmaNGmCpKQkU8dXrXtPLdW0oF1mZqbO4jcAEBERUePiN+Xl5ZUW1yEiIrJYZnr3kDkYnLR8+OGHWLNmDd58803Y2tpq27t3747jx4+bNLjqaDQazJgxA48//niNwzxKpdKgxW+Au3Nn7l9Yx5BZzURERA3OhCviWjqDk5a8vDx069atUrtcLkdJSYlJgqpNTEwMTpw4gdTUVJMfOy4uDiqVSrtdvHjR5OcgIiIiwxn89FDr1q2RnZ0NX19fnfa0tDR07NjRZIFVZ+rUqdixYwf279+Pli1b1tjXy8tL78Vv7pHL5VxvhoiIpMMMTw+Zi96VloULF6K0tBSxsbGIiYnBxo0bIYTAoUOH8PbbbyMuLg6vvfZavQUqhMDUqVOxZcsW7N69G61bt651n9DQUJ3FbwBg165dVS5+Q0REJEkCxs9nkUjSovfTQ7a2tigoKICHhwc2bNiABQsW4OzZswCA5s2bIyEhwag3N9bm5ZdfRkpKCrZt26azNotCoYCjoyMA3cVsgLuPPPfr1w+LFy/GsGHDkJqainfeeYePPBMZgE8PmR+fHjIfSTw9BMDYyIoAKGCZ13k/vZMWGxsbKJVKeHh4aNtKS0tRXFys01ZfZNX80K5btw7jxo0DAPTv3x9+fn5Yv3699vtNmzZh7ty52sXlli5datDickxayNoxaTE/Ji3mI4Wk5RpMk7S4wTKv834GJS2FhYVwd3ev75gsCpMWsnZMWsyPSYv5SCFpuQLTJC3usMzrvJ9BE3HbtWtXbcXjnmvXrhkVEBEREVFVDEpaEhISoFAo6isWIiIiMpAp1oaTyNpyhiUtzz33XIPMXyEiIiL9mGJtOImsLaf/I8+1DQsRERER1Se9Ky0SeRk0ERGRVeHwUBU0GqlcEhERkfXg8BARERGRhTH43UNERERkOTQwvlIilbEUJi1EREQSZk1zWjg8RERERJLASgsREZGEWdNEXCYtREREEmZNSQuHh4iIiEgSWGkhIiKSMGuaiMukhYiISMI4PERERERkYVhpISIikjAODxEREZEkWNOKuBweIiIiIklgpYWIiEjCrGkiLpMWIiIiCbOmOS0cHiIiIiJJYKWFiIhIwjg8RERERJJgTUkLh4eIiIhIElhpISIikjBrmojLpIWIiEjCODxEREREZGFYaSEiIpIwAeOHd4QpAmkATFqIiIgkjMNDRERERBaGlRYiIiIJs6ZKC5MWIiIiCbOmR545PERERESSwEoLERGRhHF4iIiIiCTBmpIWDg8RERGRJLDSQkREJGGciGuBEhMT0aNHDzRu3BgeHh6IjIxETk5OjfusX78eMplMZ3NwcGigiImIiOqfBn8NEdV1Y9JiYvv27UNMTAx+/vln7Nq1C7dv38bgwYNRUlJS436urq4oKCjQbhcuXGigiImIiMiUJDM8lJaWpvN5/fr18PDwQFZWFvr27VvtfjKZDF5eXnqfp7y8HOXl5drPRUVFhgdLRETUQDg8JAEqlQoA4ObmVmO/4uJi+Pr6wsfHByNGjMDJkydr7J+YmAiFQqHdfHx8TBYzERGRqRk7NGSKp48aikwIIZWXO2ppNBr87W9/w40bN3DgwIFq+2VmZuLMmTMIDAyESqXC8uXLsX//fpw8eRItW7ascp+qKi0+Pj5wBCAz9YUQEemhRHp/TT80ioqKoFAooFKp4Orqau5wdNyLLQWAk5HHKgUwGrDI67yfZIaH7hcTE4MTJ07UmLAAQGhoKEJDQ7Wfw8LC0LFjR6xevRqLFi2qch+5XA65XG7SeImIiOqLNa3TIrmkZerUqdixYwf2799fbbWkOnZ2dujWrRtyc3PrKToiIqKGxTktFkgIgalTp2LLli3YvXs3WrdubfAx1Go1jh8/Dm9v73qIkIiIiOqTZCotMTExSElJwbZt29C4cWMolUoAgEKhgKOjIwBg7NixaNGiBRITEwEACxcuRK9evRAQEIAbN25g2bJluHDhAiZOnGi26yAiIjIlDg9ZoFWrVgEA+vfvr9O+bt06jBs3DgCQn58PG5u/ikfXr1/HpEmToFQq0bRpUwQHByMjIwOdOnVqqLCJiIjqlTUlLZJ8eqgh3ZudzaeHiMhc+PSQ+Ujh6aE1MM3TQ5PAp4eIiIioHgkYP5FWKmkxkxYiIiIJs6bhIck8PURERETWjZUWIiIiCeM6LUREREQWhpUWIiIiCbOmOS1MWoiIiCTMmpIWDg8RERGRJDBpISIikjCNiTZDrVy5En5+fnBwcEBISAgOHTpUbd81a9agT58+aNq0KZo2bYrw8PAa+1eHSQsREZGEqU20GWLjxo2IjY1FfHw8jh49iq5duyIiIgKXL1+usv/evXsxatQo7NmzB5mZmfDx8cHgwYNx6dIlg87LZfxrwWX8icjcuIy/+UhhGf+lAByNPNYtAK9B/2X8Q0JC0KNHD3z00UcAAI1GAx8fH0ybNg1z5sypdX+1Wo2mTZvio48+wtixY/WOk5UWIiIiCdPA+CrLveGhoqIina28vLzS+SoqKpCVlYXw8HBtm42NDcLDw5GZmalXzKWlpbh9+zbc3NwMulYmLURERBJmyjktPj4+UCgU2i0xMbHS+a5evQq1Wg1PT0+ddk9PTyiVSr1ifv3119G8eXOdxEcffOSZiIiIAAAXL17UGR6Sy+UmP8fixYuRmpqKvXv3wsHBwaB9mbQQERFJmCnXaXF1da11TkuzZs1ga2uLwsJCnfbCwkJ4eXnVuO/y5cuxePFi/PDDDwgMDDQ4Tg4PERERSVhDP/Jsb2+P4OBgpKen/xWDRoP09HSEhoZWu9/SpUuxaNEipKWloXv37gac8S+stBAREZFBYmNjER0dje7du6Nnz55ISkpCSUkJxo8fDwAYO3YsWrRooZ0Ts2TJEsyfPx8pKSnw8/PTzn1xcXGBi4uL3udl0kJERCRh5ljGPyoqCleuXMH8+fOhVCoRFBSEtLQ07eTc/Px82Nj8NZizatUqVFRU4O9//7vOceLj47FgwQK9z8t1WmrBdVqIyNy4Tov5SGGdljcBGDadtbIyAG9D/3VazIVzWoiIiEgSODxEREQkYXV9d9CDx5ACJi1EREQSdm9FXGOPIQUcHiIiIiJJYKWFiIhIwszx9JC5MGkhIiKSMGua08LhISIiIpIEVlqIiIgkjMNDREREJAkcHiIiIiKyMKy0EBERSRiHh4iIiEgSrClp4fAQERERSQIrLURERBImYPxEWqm8R5xJCxERkYRxeIiIiIjIwrDSQkREJGHWVGlh0kJERCRhXFzOAq1atQqBgYFwdXWFq6srQkND8d1339W4z6ZNm9ChQwc4ODigS5cu+PbbbxsoWiIiIjI1ySQtLVu2xOLFi5GVlYUjR45g4MCBGDFiBE6ePFll/4yMDIwaNQoTJkzAsWPHEBkZicjISJw4caKBIyciIqo/ahNtUiATQkjlSadK3NzcsGzZMkyYMKHSd1FRUSgpKcGOHTu0bb169UJQUBCSk5P1PkdRUREUCgUcAchMETQRkYFKpPvXtOTd+x2gUqng6upq7nB03IttFAB7I49VAeA/gEVe5/0kU2m5n1qtRmpqKkpKShAaGlpln8zMTISHh+u0RUREIDMzs8Zjl5eXo6ioSGcjIiIi85PURNzjx48jNDQUZWVlcHFxwZYtW9CpU6cq+yqVSnh6euq0eXp6QqlU1niOxMREJCQkmCxmIiKi+mRNTw9JqtLSvn17ZGdn4+DBg5gyZQqio6Px22+/mfQccXFxUKlU2u3ixYsmPT4REZEpaWD8fBapPD0kqUqLvb09AgICAADBwcE4fPgw3n//faxevbpSXy8vLxQWFuq0FRYWwsvLq8ZzyOVyyOVy0wVNREREJiGpSsuDNBoNysvLq/wuNDQU6enpOm27du2qdg4MERGRFGlMtEmBZCotcXFxGDJkCFq1aoWbN28iJSUFe/fuxc6dOwEAY8eORYsWLZCYmAgAmD59Ovr164cVK1Zg2LBhSE1NxZEjR/Dxxx+b8zKIiIhMSg3jKxBSmdMimaTl8uXLGDt2LAoKCqBQKBAYGIidO3di0KBBAID8/HzY2Pz1vy0sLAwpKSmYO3cu3njjDbRt2xZbt25F586dzXUJREREZARJr9PSELhOCxGZG9dpMR8prNPyFAA7I491G8AOWP46LZKptBAREVFlfPcQERERkYVhpYWIiEjCOBGXiIiIJIHDQ0REREQWhpUWIiIiCbu3jL+xx5ACJi1EREQSpobxS3JIZU4Lh4eIiIhIElhpISIikjBrmojLpIWIiEjCODxEREREZGFYaSEiIpIwa6q0MGkhIiKSMGua08LhISIiIpIEVlqIiIgkjMNDREREJAkCxg/vCFME0gA4PERERESSwEoLERGRhJliaIfDQ0RERFTvrClp4fAQERERSQIrLURERBKmgfFPD0llnRYmLURERBLG4SEiIiIiC8NKCxERkYRZU6WFSQsREZGEWdOcFg4PERERkSSw0kJERCRhpqiSSKXSwqSFiIhIwqwpaeHwEBEREUkCKy1EREQSpobxb2mWSqWFSQsREZGEWVPSwuEhIiIikgRWWoiIiCTMmibiMmkhIiKSMA4PEREREVkYVlqIiIgkTAPjKy3G7t9QmLQQERFJmCnePSSVpEUyw0OrVq1CYGAgXF1d4erqitDQUHz33XfV9l+/fj1kMpnO5uDg0IARExERPbxWrlwJPz8/ODg4ICQkBIcOHaqx/6ZNm9ChQwc4ODigS5cu+Pbbbw0+p2SSlpYtW2Lx4sXIysrCkSNHMHDgQIwYMQInT56sdh9XV1cUFBRotwsXLjRgxERERPVPbaLNEBs3bkRsbCzi4+Nx9OhRdO3aFREREbh8+XKV/TMyMjBq1ChMmDABx44dQ2RkJCIjI3HixAmDzisTQkilKlSJm5sbli1bhgkTJlT6bv369ZgxYwZu3Lhh1DmKioqgUCjgCOPLb0REdVEi3b+mJe/e7wCVSgVXV1dzh6PjXmxOMM3wUCmg93WGhISgR48e+OijjwAAGo0GPj4+mDZtGubMmVOpf1RUFEpKSrBjxw5tW69evRAUFITk5GS945TknBa1Wo1NmzahpKQEoaGh1fYrLi6Gr68vNBoNHnvsMbzzzjt49NFHazx2eXk5ysvLtZ9VKhUA6Yz3EdHDp6ioyNwhWK17996S/31visjuHePBP2tyuRxyuVynraKiAllZWYiLi9O22djYIDw8HJmZmVUePzMzE7GxsTptERER2Lp1q0FxSippOX78OEJDQ1FWVgYXFxds2bIFnTp1qrJv+/bt8emnnyIwMBAqlQrLly9HWFgYTp48iZYtW1Z7jsTERCQkJFRqLzPZVRARGUahUJg7BKv3559/Wtz/B3t7e3h5eUGpVJrkeC4uLvDx8dFpi4+Px4IFC3Tarl69CrVaDU9PT512T09PnD59uspjK5XKKvsbGrukkpb27dsjOzsbKpUKX331FaKjo7Fv374qE5fQ0FCdKkxYWBg6duyI1atXY9GiRdWeIy4uTicbvHHjBnx9fZGfn29xf2D1UVRUBB8fH1y8eNHiSpv6YPzmxfjNT+rXIPX4VSoVWrVqBTc3N3OHUomDgwPy8vJQUVFhkuMJISCT6Q40PVhlMTdJJS329vYICAgAAAQHB+Pw4cN4//33sXr16lr3tbOzQ7du3ZCbm1tjv6pKYcDdf+lI8QfunntPXUkV4zcvxm9+Ur8GqcdvY2OZz604ODg0+JOxzZo1g62tLQoLC3XaCwsL4eXlVeU+Xl5eBvWvjmX+X9CTRqPRmX9SE7VajePHj8Pb27ueoyIiInp42dvbIzg4GOnp6do2jUaD9PT0aueZhoaG6vQHgF27dtU4L7Uqkqm0xMXFYciQIWjVqhVu3ryJlJQU7N27Fzt37gQAjB07Fi1atEBiYiIAYOHChejVqxcCAgJw48YNLFu2DBcuXMDEiRPNeRlERESSFxsbi+joaHTv3h09e/ZEUlISSkpKMH78eACVfydPnz4d/fr1w4oVKzBs2DCkpqbiyJEj+Pjjjw06r2SSlsuXL2Ps2LEoKCiAQqFAYGAgdu7ciUGDBgEA8vPzdcp3169fx6RJk6BUKtG0aVMEBwcjIyOj2om71ZHL5YiPj7e4cT19MX7zYvzmJfX4AelfA+N/OEVFReHKlSuYP38+lEolgoKCkJaWpp1s++Dv5LCwMKSkpGDu3Ll444030LZtW2zduhWdO3c26LySXqeFiIiIrIek57QQERGR9WDSQkRERJLApIWIiIgkgUkLERERSQKTlgdcu3YNY8aMgaurK5o0aYIJEyaguLi4xn369+8PmUyms7300ksNFLF5Xg9uSobEv379+kr3uqEXVrrf/v37MXz4cDRv3hwymUyv92js3bsXjz32GORyOQICArB+/fp6j7M6hsa/d+/eSvdfJpOZbBlxQyQmJqJHjx5o3LgxPDw8EBkZiZycnFr3s6Q//3W5Bkv6GVi1ahUCAwO1C8eFhobiu+++q3EfS7r/hsZvSffeWjFpecCYMWNw8uRJ7Nq1Czt27MD+/fsxefLkWvebNGkSCgoKtNvSpUsbIFrzvR7cVAyNH7i7sub99/rChQsNGLGukpISdO3aFStXrtSrf15eHoYNG4YBAwYgOzsbM2bMwMSJE7XrDTU0Q+O/JycnR+f/gYeHRz1FWL19+/YhJiYGP//8M3bt2oXbt29j8ODBKCkpqXYfS/vzX5drACznZ6Bly5ZYvHgxsrKycOTIEQwcOBAjRozAyZMnq+xvafff0PgBy7n3VkuQ1m+//SYAiMOHD2vbvvvuOyGTycSlS5eq3a9fv35i+vTpDRBhZT179hQxMTHaz2q1WjRv3lwkJiZW2X/kyJFi2LBhOm0hISHixRdfrNc4q2No/OvWrRMKhaKBojMMALFly5Ya+7z22mvi0Ucf1WmLiooSERER9RiZfvSJf8+ePQKAuH79eoPEZIjLly8LAGLfvn3V9rG0P/8P0ucaLPlnQAghmjZtKtauXVvld5Z+/4WoOX5Lv/fWgJWW+2RmZqJJkybo3r27ti08PBw2NjY4ePBgjftu2LABzZo1Q+fOnREXF4fS0tL6Dlf7evDw8HBtmz6vB7+/P3D39eDV9a9PdYkfAIqLi+Hr6wsfH59a/1VkaSzp/hsjKCgI3t7eGDRoEH766SdzhwPg7ovtANT4YjtLv//6XANgmT8DarUaqampKCkpqXZpdku+//rED1jmvbcmklkRtyEolcpKZe5GjRrBzc2txjH70aNHw9fXF82bN8evv/6K119/HTk5Odi8eXO9xmvO14ObQl3ib9++PT799FMEBgZCpVJh+fLlCAsLw8mTJ9GyZcuGCNso1d3/oqIi3Lp1C46OjmaKTD/e3t5ITk5G9+7dUV5ejrVr16J///44ePAgHnvsMbPFpdFoMGPGDDz++OM1rrBpSX/+H6TvNVjaz8Dx48cRGhqKsrIyuLi4YMuWLdWuPG6J99+Q+C3t3lsjq0ha5syZgyVLltTY59SpU3U+/v1zXrp06QJvb2888cQTOHv2LPz9/et8XKosNDRU519BYWFh6NixI1avXo1FixaZMTLr0L59e7Rv3177OSwsDGfPnsV7772Hzz//3GxxxcTE4MSJEzhw4IDZYjCWvtdgaT8D7du3R3Z2NlQqFb766itER0dj3759Br8yxVwMid/S7r01soqkZdasWRg3blyNfdq0aQMvL69KE0Dv3LmDa9euGfT67JCQEABAbm5uvSYt5nw9uCnUJf4H2dnZoVu3bsjNza2PEE2uuvvv6upq8VWW6vTs2dOsycLUqVO1k+Zr+9euJf35v58h1/Agc/8M2NvbIyAgAAAQHByMw4cP4/3338fq1asr9bXE+29I/A8y9723RlYxp8Xd3R0dOnSocbO3t0doaChu3LiBrKws7b67d++GRqPRJiL6yM7OBnC3lF6fzPl6cFOoS/wPUqvVOH78eL3fa1OxpPtvKtnZ2Wa5/0IITJ06FVu2bMHu3bvRunXrWvextPtfl2t4kKX9DGg0GpSXl1f5naXd/6rUFP+DLO3eWwVzzwS2NE8++aTo1q2bOHjwoDhw4IBo27atGDVqlPb733//XbRv314cPHhQCCFEbm6uWLhwoThy5IjIy8sT27ZtE23atBF9+/ZtkHhTU1OFXC4X69evF7/99puYPHmyaNKkiVAqlUIIIV544QUxZ84cbf+ffvpJNGrUSCxfvlycOnVKxMfHCzs7O3H8+PEGidfY+BMSEsTOnTvF2bNnRVZWlnjuueeEg4ODOHnypFniv3nzpjh27Jg4duyYACDeffddcezYMXHhwgUhhBBz5swRL7zwgrb/uXPnhJOTk5g9e7Y4deqUWLlypbC1tRVpaWmSiP+9994TW7duFWfOnBHHjx8X06dPFzY2NuKHH35o8NinTJkiFAqF2Lt3rygoKNBupaWl2j6W/ue/LtdgST8Dc+bMEfv27RN5eXni119/FXPmzBEymUx8//33VcZuafff0Pgt6d5bKyYtD/jzzz/FqFGjhIuLi3B1dRXjx48XN2/e1H6fl5cnAIg9e/YIIYTIz88Xffv2FW5ubkIul4uAgAAxe/ZsoVKpGizmDz/8ULRq1UrY29uLnj17ip9//ln7Xb9+/UR0dLRO/y+//FK0a9dO2Nvbi0cffVR88803DRZrVQyJf8aMGdq+np6eYujQoeLo0aNmiPque48AP7jdizk6Olr069ev0j5BQUHC3t5etGnTRqxbt67B474/FkPiX7JkifD39xcODg7Czc1N9O/fX+zevdsssVcVNwCd+2npf/7rcg2W9DPwz3/+U/j6+gp7e3vh7u4unnjiCe0vfCEs//4bGr8l3XtrJRNCiAYr6xARERHVkVXMaSEiIiLpY9JCREREksCkhYiIiCSBSQsRERFJApMWIiIikgQmLURERCQJTFqIiIhIEpi0EBERkSQwaSGSkP79+2PGjBlmObcQApMnT4abmxtkMpn2HVtERA2FSQuREa5cuYIpU6agVatWkMvl8PLyQkREBH766SdtH5lMhq1bt5ovyFqMGzcOkZGRtfZLS0vD+vXrsWPHDhQUFKBz584Nen4iokbmDoBIyp599llUVFTg3//+N9q0aYPCwkKkp6fjzz//NHdoJnf27Fl4e3sjLCzM3KFUSa1WQyaTwcaG/xYjemiZ+d1HRJJ1/fp1AUDs3bu32j6+vr46L8Lz9fUVQtx9EeGIESN0+k6fPl3n5YTFxcXihRdeEM7OzsLLy0ssX75c9OvXT0yfPl3bp6ysTMyaNUs0b95cODk5iZ49e2pf5imEEOvWrRMKhUKkpaWJDh06CGdnZxERESH++OMPIYQQ8fHxlV7Wd//+90RHR1d5HWq1WrzzzjvCz89PODg4iMDAQLFp0ybtfnfu3BH//Oc/td+3a9dOJCUlab+v7vz3XuR4/fp1bd97b6LOy8vTubZt27aJjh07CltbW5GXl1frPTl//rx46qmnRJMmTYSTk5Po1KmT2V8aSkT6YaWFqI5cXFzg4uKCrVu3olevXpDL5ZX6HD58GB4eHli3bh2efPJJ2Nra6n382bNnY9++fdi2bRs8PDzwxhtv4OjRowgKCtL2mTp1Kn777TekpqaiefPm2LJlC5588kkcP34cbdu2BQCUlpZi+fLl+Pzzz2FjY4Pnn38er776KjZs2IBXX30Vp06dQlFREdatWwcAcHNzqxTL+++/D39/f3z88cc4fPiw9joSExPxxRdfIDk5GW3btsX+/fvx/PPPw93dHf369YNGo0HLli2xadMmPPLII8jIyMDkyZPh7e2NkSNHVnv+jIwMve5RaWkplixZgrVr1+KRRx6Bh4dHrfckJiYGFRUV2L9/P5ydnfHbb7/BxcVF7/8vRGRG5s6aiKTsq6++Ek2bNhUODg4iLCxMxMXFiV9++UWnDwCxZcsWnbbaKi03b94U9vb24ssvv9R+/+effwpHR0dtpeXChQvC1tZWXLp0Sec4TzzxhIiLixNC3K1GABC5ubna71euXCk8PT1rjKUq7733nrbCIsTdKo+Tk5PIyMjQ6TdhwgQxatSoao8TExMjnn322RrPr2+lBYDIzs7W9tHnnnTp0kUsWLCg1uslIsvDSguREZ599lkMGzYMP/74I37++Wd89913WLp0KdauXYtx48bV+bhnz55FRUUFQkJCtG1ubm5o37699vPx48ehVqvRrl07nX3Ly8vxyCOPaD87OTnB399f+9nb2xuXL1+uc2z35ObmorS0FIMGDdJpr6ioQLdu3bSfV65ciU8//RT5+fm4desWKioqdKpFxrC3t0dgYKD2sz735JVXXsGUKVPw/fffIzw8HM8++6zOMYjIcjFpITKSg4MDBg0ahEGDBmHevHmYOHEi4uPja0xabGxsIITQabt9+7ZB5y0uLoatrS2ysrIqDTvdP9xhZ2en851MJqt07rooLi4GAHzzzTdo0aKFznf3hspSU1Px6quvYsWKFQgNDUXjxo2xbNkyHDx4sMZj35tMe3+cVd0fR0dHyGQynZhquycTJ05EREQEvvnmG3z//fdITEzEihUrMG3aNH0vnYjMhEkLkYl16tRJ5xFnOzs7qNVqnT7u7u44ceKETlt2drY2wfD394ednR0OHjyIVq1aAQCuX7+O//3vf+jXrx8AoFu3blCr1bh8+TL69OlT53jt7e0rxaePTp06QS6XIz8/XxvTg3766SeEhYXh5Zdf1radPXu21vO7u7sDAAoKCtC0aVMA0GtdGH3viY+PD1566SW89NJLiIuLw5o1a5i0EEkAnw0kqqM///wTAwcOxBdffIFff/0VeXl52LRpE5YuXYoRI0Zo+/n5+SE9PR1KpRLXr18HAAwcOBBHjhzBZ599hjNnziA+Pl4niXFxccGECRMwe/Zs7N69GydOnMC4ceN0Hudt164dxowZg7Fjx2Lz5s3Iy8vDoUOHkJiYiG+++Ubv6/Dz88Ovv/6KnJwcXL16Ve+KT+PGjfHqq69i5syZ+Pe//42zZ8/i6NGj+PDDD/Hvf/8bANC2bVscOXIEO3fuxP/+9z/MmzcPhw8frvX8AQEB8PHxwYIFC3DmzBl88803WLFiRa0x6XNPZsyYgZ07dyIvLw9Hjx7Fnj170LFjR73vFxGZkZnn1BBJVllZmZgzZ4547LHHhEKhEE5OTqJ9+/Zi7ty5orS0VNtv+/btIiAgQDRq1EhnIuv8+fOFp6enUCgUYubMmWLq1Kk6jzzfvHlTPP/888LJyUl4enqKpUuXVnrkuaKiQsyfP1/4+fkJOzs74e3tLZ5++mnx66+/CiH+eiz4flu2bBH3/+hfvnxZDBo0SLi4uFT7yLMQlSfiCiGERqMRSUlJon379sLOzk64u7uLiIgIsW/fPu09GjdunFAoFKJJkyZiypQpYs6cOaJr1661nv/AgQOiS5cuwsHBQfTp00ds2rSpykeeH1TbPZk6darw9/cXcrlcuLu7ixdeeEFcvXq1ymsmIssiE8IEg9tERERE9YzDQ0RERCQJTFqIiIhIEpi0EBERkSQwaSEiIiJJYNJCREREksCkhYiIiCSBSQsRERFJApMWIiIikgQmLURERCQJTFqIiIhIEpi0EBERkST8H7OqK5rKrPv9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Load attention weights from the JSON file\n",
    "with open('/root/khanhnnm/se/checkpoint/CMGAN_unet_32_kd_AFD_traditional_force_local_attention_ws=1/checkpoints/attn_weight/attn_weight_1.json', 'r') as f:\n",
    "    loaded_attention_weights = torch.tensor(json.load(f))\n",
    "\n",
    "# Step 4: Visualize attention weights\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(loaded_attention_weights[8].numpy(), cmap='hot', interpolation='nearest')\n",
    "plt.xlabel('Student features')\n",
    "plt.ylabel('Teacher features')\n",
    "plt.title('Attention weights')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0256\n",
      "Epoch [20/100], Loss: 1.0110\n",
      "Epoch [30/100], Loss: 1.0029\n",
      "Epoch [40/100], Loss: 0.9976\n",
      "Epoch [50/100], Loss: 0.9936\n",
      "Epoch [60/100], Loss: 0.9903\n",
      "Epoch [70/100], Loss: 0.9875\n",
      "Epoch [80/100], Loss: 0.9851\n",
      "Epoch [90/100], Loss: 0.9829\n",
      "Epoch [100/100], Loss: 0.9810\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.utils.deterministic.fill_uninitialized_memory = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Generate random data\n",
    "num_samples = 1000\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "\n",
    "X = torch.randn(num_samples, input_size)  # Random input features\n",
    "y = torch.randn(num_samples, output_size)  # Random target labels\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = SimpleNN(input_size, hidden_size=20, output_size=output_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    \n",
    "    # Compute loss (e.g., mean squared error)\n",
    "    loss = nn.MSELoss()(outputs, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/quannt40/anaconda3/envs/cmgan/lib/python3.10/site-packages/s3prl/upstream/byol_s/byol_a/common.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n",
      "ESPnet is not installed, cannot use espnet_hubert upstream\n",
      "/root/quannt40/anaconda3/envs/cmgan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/quannt40/anaconda3/envs/cmgan/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from s3prl.nn import S3PRLUpstream\n",
    "\n",
    "model = S3PRLUpstream(\"wav2vec2\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    wavs = torch.randn(2, 16000 * 2)\n",
    "    wavs_len = torch.LongTensor([16000 * 1, 16000 * 2])\n",
    "    all_hs, all_hs_len = model(wavs, wavs_len)\n",
    "\n",
    "for hs, hs_len in zip(all_hs, all_hs_len):\n",
    "    assert isinstance(hs, torch.FloatTensor)\n",
    "    assert isinstance(hs_len, torch.LongTensor)\n",
    "\n",
    "    batch_size, max_seq_len, hidden_size = hs.shape\n",
    "    print(max_seq_len)\n",
    "    assert hs_len.dim() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hs[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = all_hs[0]\n",
    "print(a.shape)\n",
    "# a = a.permute(1, 0, 2)\n",
    "conv = torch.nn.Conv1d(100, 1, 3, padding=1)\n",
    "\n",
    "out = conv(a)\n",
    "\n",
    "# out = out.permute(0, 2, 1, 3)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        print(f\"Parameter '{name}': Gradient norm = {param.grad.norm().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/quannt40/anaconda3/envs/cmgan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at patrickvonplaten/wavlm-libri-clean-100h-base-plus and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, WavLMModel\n",
    "import torch\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-base-plus\")\n",
    "model = WavLMModel.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-base-plus\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 49, 768]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file is decoded on the fly\n",
    "# Example input waveform (replace with your own waveform)\n",
    "sampling_rate = 16000\n",
    "noisy_wav = torch.randn(1, 16000)\n",
    "clean_wav = torch.randn(1, 16000)\n",
    "\n",
    "inputs = [v.unsqueeze(0).to(\"cuda\") for v in [noisy_wav, clean_wav]]\n",
    "inputs = {\"input_values\": torch.concat([noisy_wav, clean_wav]).cuda(), \n",
    "          \"attention_mask\": torch.ones(2, sampling_rate).to(torch.int32).cuda()}\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[ 1.1208,  0.5761,  0.7344,  ..., -0.7266,  0.9508,  0.1458],\n",
       "         [ 1.1123, -0.4942,  0.3371,  ...,  0.0949,  0.8816, -0.0302]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0', dtype=torch.int32)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.ones(2, sampling_rate)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_waveform = torch.randn(8, 16000)  # 1-second audio sampled at 16 kHz\n",
    "inputs = processor(input_waveform.tolist(), sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[-0.3379,  0.9558, -1.5255,  ...,  0.5001, -1.3800, -0.4833],\n",
       "        [ 0.5274,  0.0069,  0.1042,  ..., -0.9606,  1.0555,  1.1243],\n",
       "        [-1.8301, -0.2354,  0.4633,  ...,  0.3536,  0.0994, -1.6473],\n",
       "        ...,\n",
       "        [-0.5903,  0.0516,  1.0337,  ...,  0.3059, -0.3091, -0.6456],\n",
       "        [-0.2954, -0.4011,  0.0909,  ..., -0.5112,  0.6355,  0.1455],\n",
       "        [ 0.1648,  0.0377, -0.1168,  ..., -1.2678,  0.5302, -1.4015]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0', dtype=torch.int32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_waveform.tolist()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation loss: 2.0\n",
      "Gradient w.r.t x: tensor([-1.4901e-08, -7.4506e-09,  0.0000e+00,  7.4506e-09,  1.4901e-08])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between two tensors.\n",
    "    \"\"\"\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "    xm = x.sub(mean_x)\n",
    "    ym = y.sub(mean_y)\n",
    "    r_num = torch.sum(xm * ym)\n",
    "    r_den = torch.sqrt(torch.sum(xm.pow(2)) * torch.sum(ym.pow(2)))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    # To prevent division by zero\n",
    "    r = torch.clamp(r, -1.0, 1.0)\n",
    "    return r\n",
    "\n",
    "def pearson_loss(x, y):\n",
    "    \"\"\"\n",
    "    Loss based on Pearson correlation.\n",
    "    1 minus Pearson correlation to make it a minimization problem.\n",
    "    The closer the correlation is to 1, the lower the loss.\n",
    "    \"\"\"\n",
    "    r = pearson_correlation(x, y)\n",
    "    return 1 - r\n",
    "\n",
    "# Example usage\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True)\n",
    "y = torch.tensor([5.0, 4.0, 3.0, 2.0, 1.0], requires_grad=True)\n",
    "\n",
    "# Calculate Pearson loss\n",
    "loss = pearson_loss(x, y)\n",
    "print(\"Pearson correlation loss:\", loss.item())\n",
    "\n",
    "# Backpropagate the loss if needed\n",
    "loss.backward()\n",
    "print(\"Gradient w.r.t x:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def kl_divergence_batch(P, Q):\n",
    "    \"\"\"\n",
    "    Calculate the Kullback-Leibler Divergence for batches of distributions using PyTorch.\n",
    "    P and Q should be 2D tensors where each row represents a probability distribution.\n",
    "    \"\"\"\n",
    "    # Ensure no zero probability elements are used in log calculations\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    P = torch.clamp(P, epsilon, 1)\n",
    "    Q = torch.clamp(Q, epsilon, 1)\n",
    "\n",
    "    # Element-wise KL Divergence calculation\n",
    "    kl_div = torch.sum(P * torch.log(P / Q))  # Sum over features for each distribution pair\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32000])\n",
      "torch.Size([32000])\n",
      "torch.Size([32000])\n",
      "torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "audio = 'p232_005'\n",
    "\n",
    "clean_ds, _ = torchaudio.load(f'/root/se/VCTK/test/clean/{audio}.wav')\n",
    "clean_ds = clean_ds.squeeze()\n",
    "clean_ds = clean_ds[:32000]\n",
    "print(clean_ds.shape)\n",
    "\n",
    "noisy_ds, _ = torchaudio.load(f'/root/se/VCTK/test/noisy/{audio}.wav')\n",
    "noisy_ds = noisy_ds.squeeze()\n",
    "noisy_ds = noisy_ds[:32000]\n",
    "print(noisy_ds.shape)\n",
    "\n",
    "teacher_enhance_ds, _ = torchaudio.load(f'/root/se/VCTK/test/enhance/{audio}.wav')\n",
    "teacher_enhance_ds = teacher_enhance_ds.squeeze()\n",
    "teacher_enhance_ds = teacher_enhance_ds[:32000]\n",
    "print(teacher_enhance_ds.shape)\n",
    "\n",
    "\n",
    "student_enhance_ds, _ = torchaudio.load(f'/root/se/checkpoint/CMGAN_unet_32_kd_logit/checkpoints/enhanced_sample/{audio}.wav')\n",
    "student_enhance_ds = student_enhance_ds.squeeze()\n",
    "student_enhance_ds = student_enhance_ds[:32000]\n",
    "print(student_enhance_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation loss: 0.009018898010253906\n"
     ]
    }
   ],
   "source": [
    "loss = pearson_loss(clean_ds, teacher_enhance_ds)\n",
    "print(\"Pearson correlation loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.4435)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence_batch(teacher_enhance_ds, student_enhance_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0043])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence_batch(teacher_enhance_ds.unsqueeze(0), clean_ds.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0059])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence_batch(student_enhance_ds.unsqueeze(0), clean_ds.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0415])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence_batch(teacher_enhance_ds.unsqueeze(0), noisy_ds.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "alpha = 1\n",
    "\n",
    "kd_loss = torch.nn.KLDivLoss(reduction=\"sum\")(torch.nn.functional.log_softmax(teacher_enhance_ds.unsqueeze(0) / T, dim=1),\n",
    "                                torch.nn.functional.softmax(noisy_ds.unsqueeze(0) / T, dim=1)) * (alpha * T * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0033)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0172, -0.0471, -0.0222,  ..., -0.1175, -0.1331, -0.1407])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0038, -0.0062, -0.0051,  ..., -0.0835, -0.0967, -0.1007])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0017, -0.0023, -0.0020,  ..., -0.0981, -0.1101, -0.1140])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_enhance_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/quannt40/anaconda3/envs/cmgan/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): AFD(\n",
       "    (linear_trans_s): LinearTransformStudent(\n",
       "      (relu): ReLU()\n",
       "      (samplers): ModuleList()\n",
       "      (key_layer): ModuleList()\n",
       "      (bilinear): nn_bn_relu(\n",
       "        (linear): Linear(in_features=128, out_features=0, bias=True)\n",
       "        (bn): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (linear_trans_t): LinearTransformTeacher(\n",
       "      (query_layer): ModuleList()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import AFD\n",
    "from torch import nn\n",
    "\n",
    "criterion_kd_list = nn.ModuleList([])\n",
    "criterion_kd_list.append(AFD(t_shapes=[], \n",
    "                                    s_shapes=[], \n",
    "                                    qk_dim=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 5.9967e-35,  0.0000e+00, -1.9577e+27,  4.5738e-41,  0.0000e+00,\n",
       "         1.0244e-04,  0.0000e+00,  1.0736e-04,  1.3452e-43,  0.0000e+00],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_weight = torch.nn.Parameter(torch.Tensor(10))\n",
    "channel_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3497884/2116257650.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  channel_weight = torch.nn.functional.softmax(channel_weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1111, 0.1111, 0.0000, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111,\n",
       "        0.1111], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_weight = torch.nn.functional.softmax(channel_weight)\n",
    "channel_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0460, -0.7440, -0.3918,  1.6763, -0.4795, -0.4439,  0.1398,  0.4508,\n",
       "         0.3815,  0.0128], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Parameter(torch.randn(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models import generator\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from tools.compute_metrics import compute_metrics\n",
    "from utils import *\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import argparse\n",
    "from evaluation import enhance_one_track\n",
    "\n",
    "def evaluation(model_path, noisy_dir, clean_dir, save_tracks, saved_dir):\n",
    "    pesq_label_writer = open(\"/root/se/VCTK/test/pesq_label.txt\", \"w\")\n",
    "    n_fft = 400\n",
    "    model = generator.TSCNet(num_channel=64, num_features=n_fft // 2 + 1).cuda()\n",
    "    model.load_state_dict((torch.load(model_path)))\n",
    "    model.eval()\n",
    "\n",
    "    if not os.path.exists(saved_dir):\n",
    "        os.mkdir(saved_dir)\n",
    "\n",
    "    audio_list = os.listdir(noisy_dir)\n",
    "    audio_list = natsorted(audio_list)\n",
    "    num = len(audio_list)\n",
    "    metrics_total = np.zeros(6)\n",
    "    for audio in audio_list:\n",
    "        noisy_path = os.path.join(noisy_dir, audio)\n",
    "        clean_path = os.path.join(clean_dir, audio)\n",
    "        est_audio, length, _ = enhance_one_track(\n",
    "            model, noisy_path, saved_dir, 16000 * 16, n_fft, n_fft // 4, save_tracks\n",
    "        )\n",
    "        clean_audio, sr = sf.read(clean_path)\n",
    "        print(\"clean: \", clean_path)\n",
    "        assert sr == 16000\n",
    "        metrics = compute_metrics(clean_audio, est_audio, sr, 0)\n",
    "        metrics = np.array(metrics)\n",
    "        metrics_total += metrics\n",
    "        pesq_label_writer.write(\"{} {}\".format(audio, metrics[0]))\n",
    "\n",
    "    metrics_avg = metrics_total / num\n",
    "    print(\n",
    "        \"pesq: \",\n",
    "        metrics_avg[0],\n",
    "        \"csig: \",\n",
    "        metrics_avg[1],\n",
    "        \"cbak: \",\n",
    "        metrics_avg[2],\n",
    "        \"covl: \",\n",
    "        metrics_avg[3],\n",
    "        \"ssnr: \",\n",
    "        metrics_avg[4],\n",
    "        \"stoi: \",\n",
    "        metrics_avg[5],\n",
    "    )\n",
    "    pesq_label_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp:  Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)\n",
      "clean:  /root/se/VCTK/test/clean/p232_001.wav\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/se/cmgan_denoiser/src/best_ckpt/ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/se/VCTK/test/noisy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/se/VCTK/test/clean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/se/VCTK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/se/VCTK/audio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model_path, noisy_dir, clean_dir, save_tracks, saved_dir)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean: \u001b[39m\u001b[38;5;124m\"\u001b[39m, clean_path)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16000\u001b[39m\n\u001b[0;32m---> 35\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m metrics \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(metrics)\n\u001b[1;32m     37\u001b[0m metrics_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metrics\n",
      "File \u001b[0;32m~/se/cmgan_denoiser/src/tools/compute_metrics.py:40\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(cleanFile, enhancedFile, Fs, path)\u001b[0m\n\u001b[1;32m     37\u001b[0m     sampling_rate1 \u001b[38;5;241m=\u001b[39m Fs\n\u001b[1;32m     38\u001b[0m     sampling_rate2 \u001b[38;5;241m=\u001b[39m Fs\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data1) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata2\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data1), \u001b[38;5;28mlen\u001b[39m(data2))\n\u001b[1;32m     42\u001b[0m     data1 \u001b[38;5;241m=\u001b[39m data1[\u001b[38;5;241m0\u001b[39m: length] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mspacing(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluation(\"/root/se/cmgan_denoiser/src/best_ckpt/ckpt\",\n",
    "            \"/root/se/VCTK/test/noisy\",\n",
    "            \"/root/se/VCTK/test/clean\",\n",
    "            \"/root/se/VCTK\",\n",
    "\n",
    "            \"/root/se/VCTK/audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure RTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torchaudio\n",
    "\n",
    "# Assuming you have a U-Net model for speech enhancement loaded as 'model'\n",
    "# and your model takes input in the shape of (batch_size, 1, frame_size)\n",
    "\n",
    "def calculate_min_audio_chunk(n_fft, hop_size, downsampling_layers):\n",
    "    \"\"\"\n",
    "    Calculate the minimum audio chunk size needed to pass through the model.\n",
    "    \"\"\"\n",
    "    downsampling_factor = 2 ** downsampling_layers\n",
    "    min_frames = 1 * downsampling_factor\n",
    "    return (min_frames - 1) * hop_size + n_fft\n",
    "\n",
    "def measure_rtf_frame_by_frame(model, audio_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measure the Real-Time Factor (RTF) of a speech enhancement model when processing frame by frame.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The speech enhancement model (e.g., U-Net)\n",
    "    - input_audio: Input audio signal as a NumPy array or PyTorch tensor\n",
    "    - frame_size: Size of each frame in samples\n",
    "    - hop_size: Number of samples between the start of consecutive frames\n",
    "    - device: The device to run the model on ('cpu' or 'cuda')\n",
    "    \n",
    "    Returns:\n",
    "    - rtf: The calculated real-time factor\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "    n_fft = 400\n",
    "    hop_size = 100\n",
    "    downsampling_layers = 5\n",
    "\n",
    "    chunk_size = calculate_min_audio_chunk(n_fft, hop_size, downsampling_layers)\n",
    "    \n",
    "    name = os.path.split(audio_path)[-1]\n",
    "    noisy, sr = torchaudio.load(audio_path)\n",
    "    assert sr == 16000\n",
    "    print(noisy.shape)\n",
    "\n",
    "    c = torch.sqrt(noisy.size(-1) / torch.sum((noisy ** 2.0), dim=-1))\n",
    "    noisy = torch.transpose(noisy, 0, 1)\n",
    "    noisy = torch.transpose(noisy * c, 0, 1)\n",
    "\n",
    "    length = noisy.size(-1)\n",
    "    padding_len = chunk_size - (length % chunk_size)\n",
    "    noisy = torch.cat([noisy, noisy[:, :padding_len]], dim=-1)\n",
    "\n",
    "    \n",
    "    total_processing_time = 0.0\n",
    "    # Get the total duration of the input audio in seconds\n",
    "    \n",
    "    for i in range(0, length + padding_len, chunk_size):\n",
    "        frame = noisy[:, i:i + chunk_size]\n",
    "        frame = frame.to(device)\n",
    "        noisy_spec = torch.view_as_real(torch.stft(frame, n_fft, hop_size, window=torch.hamming_window(n_fft).to(device),\n",
    "                                                    onesided=True,\n",
    "                                                    return_complex=True))\n",
    "        noisy_spec = power_compress(noisy_spec).permute(0, 1, 3, 2)\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform the forward pass on the current frame\n",
    "        with torch.no_grad():\n",
    "            a, b, cc, d = model(noisy_spec)\n",
    "        \n",
    "        # Stop the timer\n",
    "        frame_processing_time = time.time() - start_time\n",
    "        total_processing_time += frame_processing_time * 1000\n",
    "    \n",
    "    # Calculate the Real-Time Factor (RTF)\n",
    "    audio_duration = chunk_size / sr  # assuming 16kHz sampling rate\n",
    "    rtf = total_processing_time / audio_duration\n",
    "    \n",
    "    return rtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.unet import UNet16, UNet32, UNet64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict_from_checkpoint(checkpoint_path):\n",
    "    state_dict = torch.load(checkpoint_path)\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:]\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "\n",
    "def load_teacher_model(checkpoint_path, n_channels):\n",
    "    state_dict = load_state_dict_from_checkpoint(checkpoint_path)\n",
    "    teacher_model = UNet64(n_channels=n_channels, bilinear=True).cuda(\"cuda:1\")\n",
    "    teacher_model.load_state_dict(state_dict)\n",
    "    return teacher_model\n",
    "\n",
    "def power_compress(x):\n",
    "    real = x[..., 0]\n",
    "    imag = x[..., 1]\n",
    "    spec = torch.complex(real, imag)\n",
    "    mag = torch.abs(spec)\n",
    "    phase = torch.angle(spec)\n",
    "    mag = mag**0.3\n",
    "    real_compress = mag * torch.cos(phase)\n",
    "    imag_compress = mag * torch.sin(phase)\n",
    "    return torch.stack([real_compress, imag_compress], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load or define your model here\n",
    "checkpoint_path = \"/root/khanhnnm/se/checkpoint/CMGAN_unet_64_bilinear_discriminator/checkpoints/best.th\"\n",
    "model = load_teacher_model(checkpoint_path, 3)  # replace with your U-Net model instance\n",
    "\n",
    "# Load or generate input audio here\n",
    "input_path = \"/root/khanhnnm/se/VCTK/test/noisy/p232_001.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27861])\n",
      "Real-Time Factor (RTF): 414.5486\n"
     ]
    }
   ],
   "source": [
    "# Measure the RTF\n",
    "rtf = measure_rtf_frame_by_frame(model, input_path, device='cuda:0')\n",
    "print(f\"Real-Time Factor (RTF): {rtf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_audio_chunk(n_fft, hop_size, downsampling_layers):\n",
    "    downsampling_factor = 2 ** downsampling_layers\n",
    "    min_frames = 1 * downsampling_factor\n",
    "    return (min_frames - 1) * hop_size + n_fft\n",
    "\n",
    "def measure_rtf(model, input_chunk, sample_rate):\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        model(input_chunk)\n",
    "    return (time.perf_counter() - start_time) / (input_chunk.shape[-1] / sample_rate)\n",
    "\n",
    "model = UNet32(3)\n",
    "chunk_size = calculate_min_audio_chunk(400, 100, 5)\n",
    "input_chunk = torch.randn(1, 3, chunk_size, 201)\n",
    "rtf = measure_rtf(model, input_chunk, 16000)\n",
    "print(f\"RTF: {rtf:.4f}, Min Chunk Size: {chunk_size} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
